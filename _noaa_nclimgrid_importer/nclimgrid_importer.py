
import polars as pl
import pyarrow.parquet as pq
import s3fs
import pyarrow.dataset as ds
import pyarrow # For pyarrow.concat_tables
import pendulum
from typing import Optional, Union, List, Tuple, Dict 
from pyarrow import Table
import re # For extracting YYYYMM from filename

def make_file_names(
        bucket: str,
        start_date: str, 
        end_date: str,   
        spatial_scale: str,
        scaled: bool,
        ) -> list[str]:
    scaled_text = 'scaled' if scaled else 'prelim'
    processed_start_date = pendulum.from_format(start_date, 'YYYY-MM-DD')
    processed_end_date = pendulum.from_format(end_date, 'YYYY-MM-DD')
    months_to_process = []
    current_month = processed_start_date.start_of('month')
    loop_until_date = processed_end_date.start_of('month')
    while current_month <= loop_until_date:
        months_to_process.append(current_month)
        current_month = current_month.add(months=1)
    file_names = [f"s3://{bucket}/EpiNOAA/v1-0-0/parquet/{spatial_scale}/YEAR={month.format('YYYY')}/STATUS={scaled_text}/{month.format('YYYYMM')}.parquet" for month in months_to_process]
    # print(f"Generated {len(file_names)} file names. First few: {file_names[:3] if file_names else 'None'}") # Less verbose
    return file_names
    
def load_nclimgrid_data(
        start_date: str,
        end_date: str,
        spatial_scale: str = 'cty', 
        scaled: bool = True,
        states: Union[List[str], str, None] = 'all', 
        counties: Union[List[str], str, None] = 'all', 
        variables: List[str] = ['tavg'] 
        ) -> Tuple[Optional[Table], List[Dict[str, str]]]:
    """
    Loads NClimGrid data from S3. Tries a batch read first. 
    If batch read fails with IndexError, falls back to individual file processing.
    """
    file_names = make_file_names(
            bucket = 'noaa-nclimgrid-daily-pds',
            start_date = start_date,
            end_date = end_date,
            spatial_scale= spatial_scale,
            scaled = scaled
            )
    
    load_errors: List[Dict[str, str]] = []

    if not file_names:
        print("  No file names generated by make_file_names, nothing to load.")
        return None, load_errors

    fs = s3fs.S3FileSystem(anon=True)
    # print(f"s3fs.S3FileSystem initialized. Type: {type(fs)}") # Less verbose
    
    known_good_schema = None
    actual_schema_column_names = []

    if file_names:
        first_file_to_test = file_names[0]
        # print(f"Attempting to directly read first file for schema: {first_file_to_test}") # Less verbose
        try:
            table = pq.read_table(first_file_to_test, filesystem=fs)
            known_good_schema = table.schema
            actual_schema_column_names = [name.lower() for name in known_good_schema.names]
            # print(f"Successfully read first file! Schema: {known_good_schema}") # Less verbose
        except Exception as e:
            err_yyyymm_match = re.search(r"(\d{6})\.parquet", first_file_to_test)
            err_yyyymm = err_yyyymm_match.group(1) if err_yyyymm_match else "SCHEMA_READ_ERROR"
            error_detail = {
                'yyyymm': err_yyyymm, 'file_path': first_file_to_test,
                'error_type': type(e).__name__, 'error_message': str(e)
            }
            load_errors.append(error_detail)
            print(f"  ERROR reading first file ({first_file_to_test}) for schema: {error_detail['error_type']} - {error_detail['error_message']}")
            return None, load_errors
    else:
        print("  file_names list is empty, cannot proceed.")
        return None, load_errors

    columns_to_select = ["date"] 
    if spatial_scale == 'cty':
        if "state_name" in actual_schema_column_names: columns_to_select.append("state_name")
        if "fips" in actual_schema_column_names: columns_to_select.append("fips")
        elif "region_name" in actual_schema_column_names: columns_to_select.append("region_name") 
    elif spatial_scale in ['ste', 'cen']: 
        if "state_name" in actual_schema_column_names: columns_to_select.append("state_name")
        if spatial_scale == 'cen' and "region_name" in actual_schema_column_names:
             if "region_name" not in columns_to_select: columns_to_select.append("region_name")

    processed_variables = []
    for var_req in variables:
        var_req_lower = var_req.lower()
        if var_req_lower in actual_schema_column_names:
            original_case_var = known_good_schema.names[actual_schema_column_names.index(var_req_lower)]
            processed_variables.append(original_case_var)
        # else: # Warning for missing variable is less critical here, handled by selection
            # print(f"Warning: Requested variable '{var_req}' not found in schema.") 
    
    columns_to_select.extend(processed_variables)
    columns_to_select = list(dict.fromkeys(columns_to_select)) 
    # print(f"Columns to select for processing: {columns_to_select}") # Less verbose

    active_filters = []
    if states != 'all' and "state_name" in actual_schema_column_names:
        states_list = states if isinstance(states, list) else [states]
        active_filters.append(ds.field("state_name").isin(states_list))
    if spatial_scale == 'cty' and counties != 'all' and "fips" in actual_schema_column_names:
        counties_list = counties if isinstance(counties, list) else [counties]
        active_filters.append(ds.field("fips").isin(counties_list))
    
    combined_filter = None
    if active_filters:
        combined_filter = active_filters[0]
        for i in range(1, len(active_filters)):
            combined_filter = combined_filter & active_filters[i]
    
    final_table = None
    # --- Attempt Batch Read First ---
    print(f"  Attempting batch read for {len(file_names)} files...")
    try:
        dataset_batch = ds.dataset(
            source=file_names,
            filesystem=fs,
            format="parquet",
            schema=known_good_schema
        )
        scanner_batch = dataset_batch.scanner(
            columns=columns_to_select,
            filter=combined_filter
        )
        final_table = scanner_batch.to_table()
        print(f"  Batch read successful. Total rows: {final_table.num_rows}")
        return final_table, load_errors # Return early if batch read works

    except IndexError as ie_batch:
        print(f"  Batch read failed with IndexError: {ie_batch}. Falling back to individual file processing.")
        # Add this batch error to load_errors for tracking
        batch_error_detail = {
            'yyyymm': 'BATCH_READ_INDEX_ERROR', 'file_path': 'N/A - Batch Attempt',
            'error_type': type(ie_batch).__name__, 'error_message': str(ie_batch)
        }
        load_errors.append(batch_error_detail)
        # Proceed to individual file processing below

    except Exception as e_batch:
        # For other errors during batch read (e.g., ConnectionError), report and return None
        print(f"  Batch read failed with other error: {type(e_batch).__name__} - {e_batch}")
        batch_error_detail = {
            'yyyymm': 'BATCH_READ_OTHER_ERROR', 'file_path': 'N/A - Batch Attempt',
            'error_type': type(e_batch).__name__, 'error_message': str(e_batch)
        }
        load_errors.append(batch_error_detail)
        return None, load_errors # Do not proceed to individual if batch fails with non-IndexError

    # --- Fallback to Individual File Processing (if batch read had IndexError) ---
    all_tables = []
    print(f"  Attempting to process {len(file_names)} files individually (fallback)...")
    for i, file_path in enumerate(file_names):
        yyyymm_match = re.search(r"(\d{6})\.parquet", file_path)
        yyyymm_str = yyyymm_match.group(1) if yyyymm_match else "UNKNOWN"
        
        if (i + 1) % 50 == 0 or i == 0 or (i + 1) == len(file_names) : 
            print(f"    Processing individual file {i+1}/{len(file_names)}: ...{file_path[-50:]}")

        try:
            single_file_dataset = ds.dataset(
                source=file_path, 
                filesystem=fs,
                format="parquet",
                schema=known_good_schema 
            )
            scanner = single_file_dataset.scanner(
                columns=columns_to_select,
                filter=combined_filter
            )
            table_chunk = scanner.to_table() 
            if table_chunk.num_rows > 0:
                all_tables.append(table_chunk)
        except Exception as e_individual: 
            error_detail = {
                'yyyymm': yyyymm_str, 'file_path': file_path,
                'error_type': type(e_individual).__name__, 'error_message': str(e_individual)
            }
            load_errors.append(error_detail)
            print(f"    ERROR processing individual file {file_path}: {error_detail['error_type']} - {error_detail['error_message']}")
            
    if not all_tables:
        print("  No data collected after individual file processing.")
        return None, load_errors

    print(f"  Concatenating {len(all_tables)} successfully processed tables (from individual processing)...")
    try:
        final_table = pyarrow.concat_tables(all_tables)
        print(f"  Successfully concatenated tables. Total rows: {final_table.num_rows}")
        return final_table, load_errors
    except Exception as e_concat:
        error_detail = {
            'yyyymm': 'CONCAT_ERROR_INDIVIDUAL', 'file_path': 'N/A',
            'error_type': type(e_concat).__name__, 'error_message': str(e_concat)
        }
        load_errors.append(error_detail)
        print(f"  Error concatenating tables from individual processing: {error_detail['error_type']} - {error_detail['error_message']}")
        return None, load_errors





# #### BELOW IS A WORKING FUNCTION, YET IT IS SLOW
# import polars as pl
# import pyarrow.parquet as pq
# import s3fs
# import pyarrow.dataset as ds
# import pyarrow # For pyarrow.concat_tables
# import pendulum
# from typing import Optional, Union, List, Tuple, Dict # Added Tuple, Dict
# from pyarrow import Table
# import re # For extracting YYYYMM from filename

# def make_file_names(
#         bucket: str,
#         start_date: str, 
#         end_date: str,   
#         spatial_scale: str,
#         scaled: bool,
#         ) -> list[str]:
#     scaled_text = 'scaled' if scaled else 'prelim'
#     processed_start_date = pendulum.from_format(start_date, 'YYYY-MM-DD')
#     processed_end_date = pendulum.from_format(end_date, 'YYYY-MM-DD')
#     months_to_process = []
#     current_month = processed_start_date.start_of('month')
#     loop_until_date = processed_end_date.start_of('month')
#     while current_month <= loop_until_date:
#         months_to_process.append(current_month)
#         current_month = current_month.add(months=1)
#     file_names = [f"s3://{bucket}/EpiNOAA/v1-0-0/parquet/{spatial_scale}/YEAR={month.format('YYYY')}/STATUS={scaled_text}/{month.format('YYYYMM')}.parquet" for month in months_to_process]
#     print(f"Generated {len(file_names)} file names. First few: {file_names[:3] if file_names else 'None'}")
#     return file_names
    
# def load_nclimgrid_data(
#         start_date: str,
#         end_date: str,
#         spatial_scale: str = 'cty', 
#         scaled: bool = True,
#         states: Union[List[str], str, None] = 'all', 
#         counties: Union[List[str], str, None] = 'all', 
#         variables: List[str] = ['tavg'] 
#         ) -> Tuple[Optional[Table], List[Dict[str, str]]]: # Return type now a tuple
#     """
#     Loads NClimGrid data from S3 for specified parameters.

#     Returns:
#         Tuple containing:
#             - Optional[pyarrow.Table]: Concatenated table of successfully processed files.
#             - List[Dict[str, str]]: A list of dictionaries, each detailing an error encountered
#                                      for a specific YYYYMM file. 
#                                      Each dict: {'yyyymm': str, 'file_path': str, 'error_type': str, 'error_message': str}
#     """
#     file_names = make_file_names(
#             bucket = 'noaa-nclimgrid-daily-pds',
#             start_date = start_date,
#             end_date = end_date,
#             spatial_scale= spatial_scale,
#             scaled = scaled
#             )
    
#     # Initialize list to store errors for this load operation
#     load_errors: List[Dict[str, str]] = []

#     if not file_names:
#         print("No file names generated by make_file_names, nothing to load.")
#         return None, load_errors

#     fs = s3fs.S3FileSystem(anon=True)
#     print(f"s3fs.S3FileSystem initialized. Type: {type(fs)}")
    
#     known_good_schema = None
#     actual_schema_column_names = []

#     if file_names:
#         first_file_to_test = file_names[0]
#         print(f"Attempting to directly read first file for schema: {first_file_to_test}")
#         try:
#             table = pq.read_table(first_file_to_test, filesystem=fs)
#             known_good_schema = table.schema
#             actual_schema_column_names = [name.lower() for name in known_good_schema.names]
#             print(f"Successfully read first file! Schema: {known_good_schema}")
#             # print(f"Actual column names (lowercase): {actual_schema_column_names}")
#             # print(f"Number of rows in first file: {table.num_rows}")
#         except Exception as e:
#             err_yyyymm_match = re.search(r"(\d{6})\.parquet", first_file_to_test)
#             err_yyyymm = err_yyyymm_match.group(1) if err_yyyymm_match else "UNKNOWN_YYYYMM"
#             error_detail = {
#                 'yyyymm': err_yyyymm,
#                 'file_path': first_file_to_test,
#                 'error_type': type(e).__name__,
#                 'error_message': str(e)
#             }
#             load_errors.append(error_detail)
#             print(f"Error reading first file ({first_file_to_test}) for schema: {error_detail['error_type']} - {error_detail['error_message']}")
#             print("Cannot proceed without a valid schema from the first file.")
#             return None, load_errors
#     else:
#         print("file_names list is empty, cannot proceed.")
#         return None, load_errors

#     columns_to_select = ["date"] 
#     if spatial_scale == 'cty':
#         if "state_name" in actual_schema_column_names: columns_to_select.append("state_name")
#         if "fips" in actual_schema_column_names: columns_to_select.append("fips")
#         elif "region_name" in actual_schema_column_names: columns_to_select.append("region_name") 
#     elif spatial_scale in ['ste', 'cen']: 
#         if "state_name" in actual_schema_column_names: columns_to_select.append("state_name")
#         if spatial_scale == 'cen' and "region_name" in actual_schema_column_names:
#              if "region_name" not in columns_to_select: columns_to_select.append("region_name")

#     processed_variables = []
#     for var_req in variables:
#         var_req_lower = var_req.lower()
#         if var_req_lower in actual_schema_column_names:
#             original_case_var = known_good_schema.names[actual_schema_column_names.index(var_req_lower)]
#             processed_variables.append(original_case_var)
#         else:
#             print(f"Warning: Requested variable '{var_req}' (as '{var_req_lower}') not found in schema: {actual_schema_column_names}")
    
#     columns_to_select.extend(processed_variables)
#     columns_to_select = list(dict.fromkeys(columns_to_select)) 
#     print(f"Columns to select for processing: {columns_to_select}")

#     active_filters = []
#     if states != 'all' and "state_name" in actual_schema_column_names:
#         states_list = states if isinstance(states, list) else [states]
#         active_filters.append(ds.field("state_name").isin(states_list))
#     if spatial_scale == 'cty' and counties != 'all' and "fips" in actual_schema_column_names:
#         counties_list = counties if isinstance(counties, list) else [counties]
#         active_filters.append(ds.field("fips").isin(counties_list))
    
#     combined_filter = None
#     if active_filters:
#         combined_filter = active_filters[0]
#         for i in range(1, len(active_filters)):
#             combined_filter = combined_filter & active_filters[i]
#         # print(f"Combined filter to apply: {combined_filter}") # Can be verbose
#     else:
#         print("No filters to apply.")

#     all_tables = []
#     print(f"\nAttempting to process {len(file_names)} files individually...")
#     for i, file_path in enumerate(file_names):
#         # Extract YYYYMM from file_path for error reporting
#         yyyymm_match = re.search(r"(\d{6})\.parquet", file_path)
#         yyyymm_str = yyyymm_match.group(1) if yyyymm_match else "UNKNOWN"
        
#         # Shortened print for less verbose output during loop
#         if (i + 1) % 10 == 0 or i == 0 or (i + 1) == len(file_names) : # Print for first, last, and every 10th file
#             print(f"Processing file {i+1}/{len(file_names)}: ...{file_path[-50:]}") # Print end of path

#         try:
#             single_file_dataset = ds.dataset(
#                 source=file_path, 
#                 filesystem=fs,
#                 format="parquet",
#                 schema=known_good_schema 
#             )
#             scanner = single_file_dataset.scanner(
#                 columns=columns_to_select,
#                 filter=combined_filter
#             )
#             table_chunk = scanner.to_table() 
#             # print(f"  Successfully read {table_chunk.num_rows} rows from {file_path} after filtering.") # Can be verbose
#             if table_chunk.num_rows > 0:
#                 all_tables.append(table_chunk)
#         except Exception as e: # Catch all exceptions for individual file processing
#             error_detail = {
#                 'yyyymm': yyyymm_str,
#                 'file_path': file_path,
#                 'error_type': type(e).__name__,
#                 'error_message': str(e)
#             }
#             load_errors.append(error_detail)
#             print(f"  ERROR processing file {file_path}: {error_detail['error_type']} - {error_detail['error_message']}")
#             # Optionally, add specific handling or re-raise for critical errors if needed
            
#     if not all_tables:
#         print("No data collected after processing all files (possibly all filtered out or errors encountered).")
#         return None, load_errors

#     print(f"\nConcatenating {len(all_tables)} successfully processed tables...")
#     try:
#         final_table = pyarrow.concat_tables(all_tables)
#         print(f"Successfully concatenated tables. Total rows: {final_table.num_rows}, Schema: {final_table.schema}")
#         return final_table, load_errors
#     except Exception as e:
#         error_detail = {
#             'yyyymm': 'CONCATENATION_ERROR',
#             'file_path': 'N/A',
#             'error_type': type(e).__name__,
#             'error_message': str(e)
#         }
#         load_errors.append(error_detail)
#         print(f"Error concatenating tables: {error_detail['error_type']} - {error_detail['error_message']}")
#         # print("Attempting to inspect schemas of collected tables if concatenation failed:")
#         # for i, t in enumerate(all_tables):
#         #     print(f"  Table {i} schema: {t.schema}")
#         return None, load_errors

#     # This line should ideally not be reached if logic is complete
#     # print("Reached end of function unexpectedly.") 
#     # return None, load_errors