{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0997cb73",
   "metadata": {},
   "source": [
    "# Load Libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022cde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b505bc",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a132c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMAX DataFrame:\n",
      "       raw_code state county division  year    Jan    Feb    Mar    Apr    May  \\\n",
      "0  01001271895    01    001       03  1895  53.70  48.70  67.60  76.40  81.90   \n",
      "1  01001271896    01    001       03  1896  54.20  60.80  65.30  81.60  88.50   \n",
      "2  01001271897    01    001       03  1897  54.20  63.10  71.40  75.10  83.20   \n",
      "3  01001271898    01    001       03  1898  60.60  59.10  71.00  72.00  89.50   \n",
      "4  01001271899    01    001       03  1899  55.60  53.40  68.80  73.40  89.30   \n",
      "\n",
      "     Jun    Jul    Aug    Sep    Oct    Nov    Dec  \n",
      "0  89.20  91.10  90.40  90.90  76.00  66.60  58.00  \n",
      "1  88.20  92.00  94.50  90.80  77.20  69.90  58.70  \n",
      "2  95.60  93.30  89.90  88.90  81.30  68.10  58.80  \n",
      "3  93.90  91.50  88.80  86.70  73.60  61.70  55.70  \n",
      "4  93.70  92.20  92.60  87.50  78.40  68.10  56.60  \n",
      "PCPN DataFrame:\n",
      "       raw_code state county division  year   Jan   Feb    Mar   Apr   May  \\\n",
      "0  01001011895    01    001       03  1895  7.03  2.96   8.36  3.53  3.96   \n",
      "1  01001011896    01    001       03  1896  5.86  5.42   5.54  3.98  3.77   \n",
      "2  01001011897    01    001       03  1897  3.27  6.63  10.94  4.35  0.81   \n",
      "3  01001011898    01    001       03  1898  2.33  2.07   2.60  4.56  0.54   \n",
      "4  01001011899    01    001       03  1899  5.80  6.94   3.35  2.22  2.93   \n",
      "\n",
      "    Jun   Jul   Aug   Sep   Oct   Nov   Dec  \n",
      "0  5.40  3.92  3.36  0.73  2.03  1.44  3.66  \n",
      "1  6.24  4.38  2.57  0.82  1.66  2.89  1.94  \n",
      "2  1.57  3.96  5.02  0.87  0.75  1.84  4.38  \n",
      "3  3.13  5.80  6.02  1.51  3.21  6.66  3.91  \n",
      "4  2.31  6.80  2.90  0.63  3.02  1.98  5.25  \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1) Read the county-to-climdivs mapping file into a lookup dict\n",
    "#    The file has 3 columns (POSTAL_FIPS_ID, NCDC_FIPS_ID, CLIMDIV_ID)\n",
    "#    We'll map NCDC_FIPS_ID -> (POSTAL_FIPS_ID, CLIMDIV_ID)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "mapping = {}\n",
    "with open(os.path.join(\"county-to-climdivs.txt\"), \"r\") as f:\n",
    "    next(f)  # skip header line if it exists\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        postal_fips, ncdc_fips, climdiv_id = parts\n",
    "        mapping[ncdc_fips] = (postal_fips, climdiv_id)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#2) Define a helper function to parse each line in tmaxcy/pcpncy\n",
    "# -----------------------------------------------------------------\n",
    "def parse_clim_line(line):\n",
    "    \"\"\"\n",
    "    Given a line (string) from tmaxcy or pcpncy,\n",
    "    returns a dict with raw_code, state, county, division, year, and the 12 monthly values.\n",
    "    If the line can't be mapped (NCDC FIPS not found), return None.\n",
    "    \"\"\"\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) < 13:\n",
    "        return None  # not enough data\n",
    "    \n",
    "    # The first item is the 11-digit code: e.g. \"01001271895\"\n",
    "    code = parts[0]\n",
    "    monthly_values = parts[1:]  # the next 12 numbers\n",
    "    \n",
    "    ncdc_fips = code[:5]       #first 5 digits\n",
    "    data_type = code[5:7]      #next 2 digits (27 for tmax, 01 for pcpn)\n",
    "    year = code[7:]            #last 4 digits\n",
    "    \n",
    "    if ncdc_fips not in mapping:\n",
    "        return None\n",
    "    \n",
    "    postal_fips, climdiv_id = mapping[ncdc_fips]\n",
    "    # postal_fips e.g. \"04001\" => correct_state=\"04\", correct_county=\"001\"\n",
    "    correct_state = postal_fips[:2]\n",
    "    correct_county = postal_fips[2:]\n",
    "    # climdiv_id e.g. \"0202\" => last two digits \"02\" for division\n",
    "    division = climdiv_id[-2:]\n",
    "    \n",
    "    # Create a dict with the data, including the raw_code for clarity\n",
    "    return {\n",
    "        \"raw_code\": code,\n",
    "        \"state\": correct_state,\n",
    "        \"county\": correct_county,\n",
    "        \"division\": division,\n",
    "        \"year\": year,\n",
    "        \"Jan\": monthly_values[0],\n",
    "        \"Feb\": monthly_values[1],\n",
    "        \"Mar\": monthly_values[2],\n",
    "        \"Apr\": monthly_values[3],\n",
    "        \"May\": monthly_values[4],\n",
    "        \"Jun\": monthly_values[5],\n",
    "        \"Jul\": monthly_values[6],\n",
    "        \"Aug\": monthly_values[7],\n",
    "        \"Sep\": monthly_values[8],\n",
    "        \"Oct\": monthly_values[9],\n",
    "        \"Nov\": monthly_values[10],\n",
    "        \"Dec\": monthly_values[11]\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#3) Read & parse tmaxcy (temperature) lines\n",
    "# -----------------------------------------------------------------\n",
    "tmax_records = []\n",
    "with open(os.path.join(\"240924 climdiv-tmaxcy-v1.0.0-20240906.txt\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        parsed = parse_clim_line(line)\n",
    "        if parsed:\n",
    "            tmax_records.append(parsed)\n",
    "\n",
    "df_tmax = pd.DataFrame(tmax_records)\n",
    "print(\"TMAX DataFrame:\\n\", df_tmax.head())\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "#4) Lastly Read & parse pcpncy (precipitation) lines\n",
    "# -----------------------------------------------------------------\n",
    "pcpn_records = []\n",
    "with open(os.path.join(\"240924 climdiv-pcpncy-v1.0.0-20240906.txt\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        parsed = parse_clim_line(line)\n",
    "        if parsed:\n",
    "            pcpn_records.append(parsed)\n",
    "\n",
    "df_pcpn = pd.DataFrame(pcpn_records)\n",
    "print(\"PCPN DataFrame:\\n\", df_pcpn.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbf210",
   "metadata": {},
   "source": [
    "#### Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee6404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Temperature Data (per month):\n",
      "Series([], dtype: float64)\n",
      "\n",
      "Missing values in Precipitation Data (per month):\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data in temperature and precipitation DataFrames\n",
    "#identify monthly columns by suffix in temperature and precipitation DataFrames\n",
    "tmax_months = [col for col in df_tmax.columns if col.endswith(\"_tmax\")]\n",
    "pcpn_months = [col for col in df_pcpn.columns if col.endswith(\"_pcpn\")]\n",
    "\n",
    "#count missing values in temperature data (-99.99 indicates missing)\n",
    "missing_tmax = (df_tmax[tmax_months] == -99.99).sum()\n",
    "print(\"Missing values in Temperature Data (per month):\")\n",
    "print(missing_tmax)\n",
    "\n",
    "#count missing values in precipitation data (-9.99 indicates missing)\n",
    "missing_pcpn = (df_pcpn[pcpn_months] == -9.99).sum()\n",
    "print(\"\\nMissing values in Precipitation Data (per month):\")\n",
    "print(missing_pcpn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab574da9",
   "metadata": {},
   "source": [
    "# Align with corn yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2827cd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state_ansi county_ansi district_code  \\\n",
      "0  2023         17         107            04   \n",
      "1  2023         17         115            04   \n",
      "2  2023         17         125            04   \n",
      "3  2023         17         113            04   \n",
      "4  2023         17         129            04   \n",
      "\n",
      "                                    data_item  value   cv  \n",
      "0  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE  211.1  1.6  \n",
      "1  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE  225.3  2.8  \n",
      "2  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE  208.1  3.6  \n",
      "3  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE  223.3  2.2  \n",
      "4  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE  214.4  3.9  \n"
     ]
    }
   ],
   "source": [
    "# Load corn yield data\n",
    "\n",
    "\n",
    "# Join path and filename correctly\n",
    "path_corn = \"../_corn_yield_data/\"\n",
    "filename = \"df_corn_yield.csv\"\n",
    "full_path = os.path.join(path_corn, filename)\n",
    "\n",
    "\n",
    "df_corn_yield = pd.read_csv(\n",
    "    full_path,\n",
    "    dtype={\n",
    "        'state_ansi': str,\n",
    "        'county_ansi': str,\n",
    "        'district_code': str\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df_corn_yield.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe0cc7",
   "metadata": {},
   "source": [
    "### Start pre-merging diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117563a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Temperature DataFrame States: ['17' '19' '27' '31']\n",
      "Filtered Precipitation DataFrame States: ['17' '19' '27' '31']\n"
     ]
    }
   ],
   "source": [
    "# Filter the temperature and precipitation DataFrames to include only states in the corn yield data\n",
    "# Here we reuse the unique_states from the crop yield DataFrame as the allowed state list\n",
    "allowed_states = df_corn_yield[\"state_ansi\"].astype(str).unique()\n",
    "\n",
    "df_tmax = df_tmax[df_tmax[\"state\"].isin(allowed_states)]\n",
    "df_pcpn = df_pcpn[df_pcpn[\"state\"].isin(allowed_states)]\n",
    "\n",
    "print(\"Filtered Temperature DataFrame States:\", df_tmax[\"state\"].unique())\n",
    "print(\"Filtered Precipitation DataFrame States:\", df_pcpn[\"state\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89e797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counties per state in df_tmax:\n",
      "state\n",
      "17    102\n",
      "19     99\n",
      "27     87\n",
      "31     93\n",
      "Name: county, dtype: int64\n",
      "\n",
      "Unique counties per state in df_corn_yield:\n",
      "state_ansi\n",
      "17    102\n",
      "19     99\n",
      "27     85\n",
      "31     93\n",
      "Name: county_ansi, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count unique counties per state in the temperature DataFrame:\n",
    "tmax_counties_per_state = df_tmax.groupby('state')['county'].nunique()\n",
    "print(\"Unique counties per state in df_tmax:\")\n",
    "print(tmax_counties_per_state)\n",
    "\n",
    "# Count unique counties per state in the corn yield DataFrame:\n",
    "corn_counties_per_state = df_corn_yield.groupby('state_ansi')['county_ansi'].nunique()\n",
    "print(\"\\nUnique counties per state in df_corn_yield:\")\n",
    "print(corn_counties_per_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0664cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year ranges per state:\n",
      "             min   max\n",
      "state_ansi            \n",
      "17          1926  2023\n",
      "19          1926  2023\n",
      "27          1926  2023\n",
      "31          1926  2023\n",
      "Highest minimum year across states: 1926\n",
      "Lowest maximum year across states: 2023\n",
      "Filtered df_tmax:\n",
      "           raw_code state county division  year    Jan    Feb    Mar    Apr  \\\n",
      "72831   11001271926    17    001       03  1926  37.00  43.10  45.00  57.00   \n",
      "72832   11001271927    17    001       03  1927  32.80  46.10  53.90  62.00   \n",
      "72833   11001271928    17    001       03  1928  36.60  42.00  53.80  60.20   \n",
      "72834   11001271929    17    001       03  1929  29.70  31.80  56.80  64.40   \n",
      "72835   11001271930    17    001       03  1930  26.20  49.20  51.20  70.00   \n",
      "...             ...   ...    ...      ...   ...    ...    ...    ...    ...   \n",
      "222554  25185272019    31    185       06  2019  34.00  27.30  42.40  65.40   \n",
      "222555  25185272020    31    185       06  2020  33.70  41.90  53.00  63.10   \n",
      "222556  25185272021    31    185       06  2021  37.60  22.80  56.50  62.70   \n",
      "222557  25185272022    31    185       06  2022  38.40  42.00  53.40  63.80   \n",
      "222558  25185272023    31    185       06  2023  35.30  41.50  47.60  66.80   \n",
      "\n",
      "          May    Jun    Jul    Aug    Sep    Oct    Nov    Dec  \n",
      "72831   78.80  81.00  88.60  86.30  75.40  63.50  45.50  36.30  \n",
      "72832   71.60  77.90  86.40  81.80  82.60  71.10  53.40  36.90  \n",
      "72833   76.80  77.00  87.90  86.50  75.20  69.30  49.30  41.40  \n",
      "72834   70.40  81.30  87.00  85.60  76.70  66.20  44.80  38.40  \n",
      "72835   76.20  84.30  93.60  91.10  83.70  64.50  54.00  38.00  \n",
      "...       ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "222554  68.40  82.00  87.10  81.90  82.00  59.30  48.20  41.20  \n",
      "222555  68.60  87.60  86.20  85.20  76.10  61.10  57.70  42.10  \n",
      "222556  71.00  87.10  85.80  87.00  82.30  67.90  55.40  48.00  \n",
      "222557  72.50  86.60  87.60  88.80  82.70  68.20  49.20  33.20  \n",
      "222558  78.70  87.20  85.20  87.80  84.40  66.60  55.40  43.60  \n",
      "\n",
      "[37338 rows x 17 columns]\n",
      "\n",
      "Filtered df_pcpn:\n",
      "           raw_code state county division  year   Jan   Feb   Mar   Apr   May  \\\n",
      "72831   11001011926    17    001       03  1926  1.32  1.98  2.61  2.98  2.15   \n",
      "72832   11001011927    17    001       03  1927  1.49  1.30  4.91  5.76  4.93   \n",
      "72833   11001011928    17    001       03  1928  0.43  1.67  1.20  3.06  2.50   \n",
      "72834   11001011929    17    001       03  1929  3.61  0.76  4.57  5.46  5.74   \n",
      "72835   11001011930    17    001       03  1930  3.31  1.41  1.20  2.04  2.27   \n",
      "...             ...   ...    ...      ...   ...   ...   ...   ...   ...   ...   \n",
      "222554  25185012019    31    185       06  2019  0.20  1.11  2.97  1.30  7.88   \n",
      "222555  25185012020    31    185       06  2020  1.22  0.07  2.29  0.98  5.05   \n",
      "222556  25185012021    31    185       06  2021  1.39  0.69  7.31  1.59  4.57   \n",
      "222557  25185012022    31    185       06  2022  0.16  0.03  1.20  2.13  5.34   \n",
      "222558  25185012023    31    185       06  2023  1.12  1.07  0.80  1.39  0.76   \n",
      "\n",
      "         Jun   Jul   Aug    Sep   Oct   Nov   Dec  \n",
      "72831   7.09  3.07  4.99  12.45  3.36  3.36  1.09  \n",
      "72832   4.64  2.90  2.43   3.16  4.69  2.94  2.48  \n",
      "72833   5.28  3.29  3.09   4.05  4.38  4.52  1.62  \n",
      "72834   5.02  6.95  2.26   3.14  4.27  1.68  0.84  \n",
      "72835   4.66  0.41  1.54   2.95  2.54  3.62  0.56  \n",
      "...      ...   ...   ...    ...   ...   ...   ...  \n",
      "222554  5.37  4.63  6.85   2.02  2.53  1.29  1.79  \n",
      "222555  2.87  5.26  1.21   1.69  0.56  1.74  0.89  \n",
      "222556  2.98  1.94  4.45   1.55  2.91  0.49  0.27  \n",
      "222557  3.77  3.57  1.07   1.86  0.37  0.32  0.32  \n",
      "222558  2.18  6.16  2.07   1.19  1.44  0.61  1.57  \n",
      "\n",
      "[37338 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "## --- CHECK HIGHEST MIN AND LOWEST MAX YEAR FOR CORN YIELD DATA ---\n",
    "# Compute the min and max year for each state in the corn yield dataset\n",
    "state_year_ranges = df_corn_yield.groupby(\"state_ansi\")[\"year\"].agg([\"min\", \"max\"])\n",
    "print(\"Year ranges per state:\")\n",
    "print(state_year_ranges)\n",
    "\n",
    "# Get the highest minimum year (i.e. the maximum of the minimum years)\n",
    "highest_min_year = state_year_ranges[\"min\"].max()\n",
    "\n",
    "# Get the lowest maximum year (i.e. the minimum of the maximum years)\n",
    "lowest_max_year = state_year_ranges[\"max\"].min()\n",
    "\n",
    "print(f\"Highest minimum year across states: {highest_min_year}\")\n",
    "print(f\"Lowest maximum year across states: {lowest_max_year}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "## --- APPLY TO WEATHER DATA ---\n",
    "# Convert the 'year' column to int (if not already) and filter the data frames\n",
    "df_tmax[\"year\"] = df_tmax[\"year\"].astype(int)\n",
    "df_tmax = df_tmax[(df_tmax[\"year\"] >= highest_min_year) & (df_tmax[\"year\"] <= lowest_max_year)]\n",
    "\n",
    "df_pcpn[\"year\"] = df_pcpn[\"year\"].astype(int)\n",
    "df_pcpn = df_pcpn[(df_pcpn[\"year\"] >= highest_min_year) & (df_pcpn[\"year\"] <= lowest_max_year)]\n",
    "\n",
    "print(\"Filtered df_tmax:\")\n",
    "print(df_tmax.head(100000))\n",
    "print(\"\\nFiltered df_pcpn:\")\n",
    "print(df_pcpn.head(100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd60710",
   "metadata": {},
   "source": [
    "### Make growing season variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee4e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
      "Growing season temperature by year, state, county, and division:\n",
      "   year state county division    temp_gs\n",
      "0  1926    17    001       03  25.472222\n",
      "1  1926    17    003       08  27.944444\n",
      "2  1926    17    005       06  26.324074\n",
      "3  1926    17    007       02  22.277778\n",
      "4  1926    17    009       03  25.222222\n",
      "\n",
      "Growing season precipitation by year, state, county, and division:\n",
      "   year state county division  pcpn_gs\n",
      "0  1926    17    001       03  831.342\n",
      "1  1926    17    003       08  497.840\n",
      "2  1926    17    005       06  610.616\n",
      "3  1926    17    007       02  708.660\n",
      "4  1926    17    009       03  861.314\n"
     ]
    }
   ],
   "source": [
    "# Be aware:\n",
    "# - Get params from config file\n",
    "# - convert to SI units\n",
    "\n",
    "import calendar\n",
    "# Add parent directory to Python path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "# Now import config\n",
    "import config\n",
    "\n",
    "# This creates the list of standard month abbreviations\n",
    "# e.g., ['', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_name_list = calendar.month_abbr \n",
    "\n",
    "# This uses the start and end month numbers from your config to slice the list\n",
    "# GS_START_MONTH (e.g., 4) becomes the start of the slice\n",
    "# GS_END_MONTH + 1 (e.g., 9 + 1 = 10) becomes the end of the slice (exclusive)\n",
    "months = list(month_name_list[config.GS_START_MONTH : config.GS_END_MONTH + 1])\n",
    "print(months)\n",
    "\n",
    "# --- Temperature Calculation ---\n",
    "# Convert the monthly temperature columns to numeric values\n",
    "df_tmax[months] = df_tmax[months].apply(pd.to_numeric, errors='coerce')\n",
    "# Convert Fahrenheit to Celsius: °C = (°F - 32) × 5/9\n",
    "df_tmax[months] = (df_tmax[months] - 32) * 5 / 9\n",
    "# Compute the arithmetic mean (across the months) for each row\n",
    "df_tmax['temp_gs'] = df_tmax[months].mean(axis=1)\n",
    "# Group by year, state, county, and division to get one value per group\n",
    "df_temp_growing_season = df_tmax.groupby(\n",
    "    ['year', 'state', 'county', 'division']\n",
    ")['temp_gs'].mean().reset_index()\n",
    "\n",
    "print(\"Growing season temperature by year, state, county, and division:\")\n",
    "print(df_temp_growing_season.head())\n",
    "\n",
    "# --- Precipitation Calculation ---\n",
    "# Convert the monthly precipitation columns to numeric values\n",
    "df_pcpn[months] = df_pcpn[months].apply(pd.to_numeric, errors='coerce')\n",
    "# Convert inches to millimeters: mm = inches × 25.4\n",
    "df_pcpn[months] = df_pcpn[months] * 25.4\n",
    "# Compute the total precipitation (sum over the months) for each row\n",
    "df_pcpn['pcpn_gs'] = df_pcpn[months].sum(axis=1)\n",
    "# Group by year, state, county, and division to get one value per group\n",
    "df_precip_growing_season = df_pcpn.groupby(\n",
    "    ['year', 'state', 'county', 'division']\n",
    ")['pcpn_gs'].mean().reset_index()\n",
    "\n",
    "print(\"\\nGrowing season precipitation by year, state, county, and division:\")\n",
    "print(df_precip_growing_season.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b42ed",
   "metadata": {},
   "source": [
    "#### Check one more time if nothing is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63a8e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing data\n"
     ]
    }
   ],
   "source": [
    "# Use the corn yield year range as the common complete years\n",
    "complete_years = set(range(highest_min_year, lowest_max_year + 1))\n",
    "\n",
    "# Get the unique (state, county) keys present in either temperature or precipitation datasets\n",
    "keys_temp = set(df_temp_growing_season.groupby([\"state\", \"county\"]).groups.keys())\n",
    "keys_pcpn = set(df_precip_growing_season.groupby([\"state\", \"county\"]).groups.keys())\n",
    "all_keys = keys_temp.union(keys_pcpn)\n",
    "\n",
    "missing_weather_report = {}\n",
    "\n",
    "for key in all_keys:\n",
    "    state, county = key\n",
    "    # Get years available from the temperature dataset for the county (if any)\n",
    "    years_temp = set(\n",
    "        df_temp_growing_season.loc[\n",
    "            (df_temp_growing_season[\"state\"] == state) & (df_temp_growing_season[\"county\"] == county),\n",
    "            \"year\"\n",
    "        ].astype(int).unique()\n",
    "    )\n",
    "    # Get years available from the precipitation dataset for the county (if any)\n",
    "    years_pcpn = set(\n",
    "        df_precip_growing_season.loc[\n",
    "            (df_precip_growing_season[\"state\"] == state) & (df_precip_growing_season[\"county\"] == county),\n",
    "            \"year\"\n",
    "        ].astype(int).unique()\n",
    "    )\n",
    "    \n",
    "    # Combine the available years from both data types\n",
    "    years_present = years_temp.union(years_pcpn)\n",
    "    missing_rows = sorted(complete_years - years_present)\n",
    "    \n",
    "    if missing_rows:\n",
    "        missing_weather_report[(state, county)] = missing_rows\n",
    "\n",
    "# Print the missing report in sorted order if any missing data exists, else print 'no missing data'\n",
    "if missing_weather_report:\n",
    "    for state, county in sorted(missing_weather_report.keys()):\n",
    "        print(f\"State {state}, County {county}:\")\n",
    "        print(f\"  Rows missing for years: {missing_weather_report[(state, county)]}\")\n",
    "else:\n",
    "    print(\"no missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacfca3",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431611cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state county division_x                                   data_item  \\\n",
      "0  1926    17    001         03  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "1  1927    17    001         03  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "2  1928    17    001         03  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "3  1929    17    001         03  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "4  1930    17    001         03  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "\n",
      "   value  cv division_y    temp_gs division  pcpn_gs  \n",
      "0   38.0 NaN         03  25.472222       03  831.342  \n",
      "1   30.0 NaN         03  25.027778       03  605.028  \n",
      "2   41.0 NaN         03  25.148148       03  540.258  \n",
      "3   31.0 NaN         03  25.314815       03  725.678  \n",
      "4   29.0 NaN         03  28.416667       03  352.298  \n"
     ]
    }
   ],
   "source": [
    "# Example: Load your dataframes (replace these with your actual file loading commands)\n",
    "# corn_yield_df = pd.read_csv(\"corn_yield.csv\")\n",
    "# df_temp_growing_season = pd.read_csv(\"temp_growing_season.csv\")\n",
    "# df_precip_growing_season = pd.read_csv(\"precip_growing_season.csv\")\n",
    "\n",
    "# Rename columns in corn_yield_df to match the keys in the other dataframes\n",
    "corn_yield_df = df_corn_yield.rename(columns={\n",
    "    'state_ansi': 'state',\n",
    "    'county_ansi': 'county',\n",
    "    'district_code': 'division'\n",
    "}).copy()\n",
    "\n",
    "# Ensure that the key columns 'state', 'county', and 'year' have the same data types\n",
    "# In this example, we'll convert 'state' and 'county' to string.\n",
    "for df in [corn_yield_df, df_temp_growing_season, df_precip_growing_season]:\n",
    "    df['state'] = df['state'].astype(str)\n",
    "    df['county'] = df['county'].astype(str)\n",
    "    # Assuming 'year' is consistent (e.g., int) between datasets; if not, convert as needed:\n",
    "    # df['year'] = df['year'].astype(int)\n",
    "\n",
    "# Merge corn_yield_df with the temperature dataframe using an outer join.\n",
    "merged_df = pd.merge(\n",
    "    corn_yield_df,\n",
    "    df_temp_growing_season,\n",
    "    on=['state', 'county', 'year'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge the resulting dataframe with the precipitation dataframe using an outer join.\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    df_precip_growing_season,\n",
    "    on=['state', 'county', 'year'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bad722",
   "metadata": {},
   "source": [
    "##### Fix the triple division presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b84a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found between division_x and division_y in some rows.\n",
      "     division_x division_y\n",
      "9702         08         09\n",
      "9703         08         09\n",
      "9704         08         09\n",
      "9705         08         09\n",
      "9706         08         09\n",
      "...         ...        ...\n",
      "9794         08         09\n",
      "9795         08         09\n",
      "9796         08         09\n",
      "9798         08         09\n",
      "9799         08         09\n",
      "\n",
      "[97 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check only rows where division_x is not NaN\n",
    "mask = merged_df[\"division_x\"].notna()\n",
    "\n",
    "# Evaluate whether division_x equals division_y in those rows\n",
    "if (merged_df.loc[mask, \"division_x\"] == merged_df.loc[mask, \"division_y\"]).all():\n",
    "    print(\"All non-NaN division_x values match division_y.\")\n",
    "else:\n",
    "    print(\"Mismatch found between division_x and division_y in some rows.\")\n",
    "    # Optionally, print rows with mismatches for inspection:\n",
    "    mismatches = merged_df.loc[mask][merged_df.loc[mask, \"division_x\"] != merged_df.loc[mask, \"division_y\"]]\n",
    "    print(mismatches[[\"division_x\", \"division_y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6447f",
   "metadata": {},
   "source": [
    "After inspection of the df it was found that for state 17, county 199, the division is 08 for the yield data and 09 for the NOAA data. A quick inspection of the 'county-to-climdivs' and '240917_corn_yield_data' shows that this is the case from the start and not a coding error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1e0259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-NaN values in division_y match division.\n"
     ]
    }
   ],
   "source": [
    "# Check if all non-NaN values in division_y match division\n",
    "mask_div = merged_df[\"division_y\"].notna()\n",
    "\n",
    "if (merged_df.loc[mask_div, \"division_y\"] == merged_df.loc[mask_div, \"division\"]).all():\n",
    "    print(\"All non-NaN values in division_y match division.\")\n",
    "else:\n",
    "    print(\"Mismatch found between division_y and division in some rows.\")\n",
    "    mismatches = merged_df.loc[mask_div][\n",
    "        merged_df.loc[mask_div, \"division_y\"] != merged_df.loc[mask_div, \"division\"]\n",
    "    ]\n",
    "    print(mismatches[[\"division_y\", \"division\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e2bf7",
   "metadata": {},
   "source": [
    "##### Neatly finalize divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2031480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year state county division_yield division_noaa  \\\n",
      "0  1926    17    001             03            03   \n",
      "1  1927    17    001             03            03   \n",
      "2  1928    17    001             03            03   \n",
      "3  1929    17    001             03            03   \n",
      "4  1930    17    001             03            03   \n",
      "\n",
      "                                    data_item  value  cv    temp_gs  pcpn_gs  \n",
      "0  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   38.0 NaN  25.472222  831.342  \n",
      "1  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   30.0 NaN  25.027778  605.028  \n",
      "2  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   41.0 NaN  25.148148  540.258  \n",
      "3  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   31.0 NaN  25.314815  725.678  \n",
      "4  CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   29.0 NaN  28.416667  352.298  \n"
     ]
    }
   ],
   "source": [
    "# Assume merged_df is already created via prior merging steps\n",
    "\n",
    "# Create a copy of merged_df and work with the new DataFrame final_merged_df\n",
    "final_merged_df = merged_df.copy()\n",
    "\n",
    "# 1. Rename 'division_x' to 'division_yield'\n",
    "final_merged_df.rename(columns={'division_x': 'division_yield'}, inplace=True)\n",
    "\n",
    "# 2. Create new column 'division_noaa'\n",
    "# Since you've verified that division_y and division are essentially the same,\n",
    "# we fill from division_y and fallback to division if necessary.\n",
    "final_merged_df['division_noaa'] = final_merged_df['division_y'].fillna(final_merged_df['division'])\n",
    "\n",
    "# 3. Drop the now redundant columns 'division_y' and 'division'\n",
    "final_merged_df.drop(columns=['division_y', 'division'], inplace=True)\n",
    "\n",
    "# 4. Reorder columns so that 'division_noaa' is placed immediately after 'division_yield'\n",
    "cols = list(final_merged_df.columns)\n",
    "# Find index of 'division_yield'\n",
    "idx = cols.index('division_yield')\n",
    "# Build the new column order:\n",
    "new_cols = cols[:idx+1] + ['division_noaa'] + cols[idx+1:]\n",
    "# In case 'division_noaa' appears twice, remove duplicates while preserving order\n",
    "new_cols = list(dict.fromkeys(new_cols))\n",
    "final_merged_df = final_merged_df[new_cols]\n",
    "\n",
    "# Display the first few rows to verify the ordering\n",
    "print(final_merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05869ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'value', 'temp_gs', and 'pcpn_gs' columns are the same in both DataFrames: True\n"
     ]
    }
   ],
   "source": [
    "## --- Check if the variable values of the df with new division set-up are the same as before ---\n",
    "\n",
    "# Define the columns to check\n",
    "cols_to_check = ['value', 'temp_gs', 'pcpn_gs']\n",
    "\n",
    "# Compare the corresponding columns between merged_df and final_merged_df\n",
    "are_equal = merged_df[cols_to_check].equals(final_merged_df[cols_to_check])\n",
    "print(\"The 'value', 'temp_gs', and 'pcpn_gs' columns are the same in both DataFrames:\", are_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be55ade",
   "metadata": {},
   "source": [
    "### Save the created df containing both crop yield data and weather regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4e7bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to df_yield_weather.csv\n"
     ]
    }
   ],
   "source": [
    "# Assuming final_merged_df is your final DataFrame that you want to save.\n",
    "# For example:\n",
    "# final_merged_df = merged_df  (after doing all your renaming/reordering)\n",
    "\n",
    "# Convert key columns to string to preserve leading zeros!!!!\n",
    "for col in ['county', 'division_yield', 'division_noaa']:\n",
    "    final_merged_df[col] = final_merged_df[col].astype(str)\n",
    "\n",
    "save_file = \"df_yield_climdiv.csv\"\n",
    "\n",
    "final_merged_df.to_csv(save_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(\"Data saved to df_yield_weather.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
